{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79a864d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting crawl4ai\n",
      "  Downloading crawl4ai-0.5.0.post8-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting aiosqlite~=0.20 (from crawl4ai)\n",
      "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: lxml~=5.3 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from crawl4ai) (5.3.2)\n",
      "Collecting litellm>=1.53.1 (from crawl4ai)\n",
      "  Downloading litellm-1.66.1-py3-none-any.whl.metadata (36 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.26.0 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from crawl4ai) (2.1.3)\n",
      "Collecting pillow~=10.4 (from crawl4ai)\n",
      "  Using cached pillow-10.4.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: playwright>=1.49.0 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from crawl4ai) (1.51.0)\n",
      "Requirement already satisfied: python-dotenv~=1.0 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from crawl4ai) (1.1.0)\n",
      "Requirement already satisfied: requests~=2.26 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from crawl4ai) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4~=4.12 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from crawl4ai) (4.13.4)\n",
      "Collecting tf-playwright-stealth>=1.1.0 (from crawl4ai)\n",
      "  Downloading tf_playwright_stealth-1.1.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: xxhash~=3.4 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from crawl4ai) (3.5.0)\n",
      "Collecting rank-bm25~=0.2 (from crawl4ai)\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting aiofiles>=24.1.0 (from crawl4ai)\n",
      "  Using cached aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting colorama~=0.4 (from crawl4ai)\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting snowballstemmer~=2.2 (from crawl4ai)\n",
      "  Using cached snowballstemmer-2.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pydantic>=2.10 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from crawl4ai) (2.11.3)\n",
      "Collecting pyOpenSSL>=24.3.0 (from crawl4ai)\n",
      "  Downloading pyOpenSSL-25.0.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: psutil>=6.1.1 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from crawl4ai) (7.0.0)\n",
      "Collecting nltk>=3.9.1 (from crawl4ai)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: rich>=13.9.4 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from crawl4ai) (14.0.0)\n",
      "Collecting cssselect>=1.2.0 (from crawl4ai)\n",
      "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: httpx>=0.27.2 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from crawl4ai) (0.28.1)\n",
      "Collecting fake-useragent>=2.0.3 (from crawl4ai)\n",
      "  Downloading fake_useragent-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: click>=8.1.7 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from crawl4ai) (8.1.8)\n",
      "Collecting pyperclip>=1.8.2 (from crawl4ai)\n",
      "  Downloading pyperclip-1.9.0.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting faust-cchardet>=2.1.19 (from crawl4ai)\n",
      "  Downloading faust_cchardet-2.1.19-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: aiohttp>=3.11.11 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from crawl4ai) (3.11.16)\n",
      "Collecting humanize>=4.10.0 (from crawl4ai)\n",
      "  Downloading humanize-4.12.2-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from aiohttp>=3.11.11->crawl4ai) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from aiohttp>=3.11.11->crawl4ai) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from aiohttp>=3.11.11->crawl4ai) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from aiohttp>=3.11.11->crawl4ai) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from aiohttp>=3.11.11->crawl4ai) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from aiohttp>=3.11.11->crawl4ai) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from aiohttp>=3.11.11->crawl4ai) (1.19.0)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from aiosqlite~=0.20->crawl4ai) (4.13.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from beautifulsoup4~=4.12->crawl4ai) (2.6)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from httpx>=0.27.2->crawl4ai) (4.9.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from httpx>=0.27.2->crawl4ai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from httpx>=0.27.2->crawl4ai) (1.0.8)\n",
      "Requirement already satisfied: idna in /Users/binbasri0/.local/lib/python3.11/site-packages (from httpx>=0.27.2->crawl4ai) (3.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.27.2->crawl4ai) (0.14.0)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from litellm>=1.53.1->crawl4ai) (8.6.1)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from litellm>=1.53.1->crawl4ai) (3.1.6)\n",
      "Collecting jsonschema<5.0.0,>=4.22.0 (from litellm>=1.53.1->crawl4ai)\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting openai>=1.68.2 (from litellm>=1.53.1->crawl4ai)\n",
      "  Downloading openai-1.75.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting tiktoken>=0.7.0 (from litellm>=1.53.1->crawl4ai)\n",
      "  Downloading tiktoken-0.9.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tokenizers in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from litellm>=1.53.1->crawl4ai) (0.21.1)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from nltk>=3.9.1->crawl4ai) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from nltk>=3.9.1->crawl4ai) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from nltk>=3.9.1->crawl4ai) (4.67.1)\n",
      "Requirement already satisfied: pyee<13,>=12 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from playwright>=1.49.0->crawl4ai) (12.1.1)\n",
      "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from playwright>=1.49.0->crawl4ai) (3.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from pydantic>=2.10->crawl4ai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from pydantic>=2.10->crawl4ai) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from pydantic>=2.10->crawl4ai) (0.4.0)\n",
      "Collecting cryptography<45,>=41.0.5 (from pyOpenSSL>=24.3.0->crawl4ai)\n",
      "  Downloading cryptography-44.0.2-cp39-abi3-macosx_10_9_universal2.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from requests~=2.26->crawl4ai) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from requests~=2.26->crawl4ai) (2.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from rich>=13.9.4->crawl4ai) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from rich>=13.9.4->crawl4ai) (2.19.1)\n",
      "Collecting fake-http-header<0.4.0,>=0.3.5 (from tf-playwright-stealth>=1.1.0->crawl4ai)\n",
      "  Downloading fake_http_header-0.3.5-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting cffi>=1.12 (from cryptography<45,>=41.0.5->pyOpenSSL>=24.3.0->crawl4ai)\n",
      "  Downloading cffi-1.17.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm>=1.53.1->crawl4ai) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.53.1->crawl4ai) (3.0.2)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai)\n",
      "  Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai)\n",
      "  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai)\n",
      "  Downloading rpds_py-0.24.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->crawl4ai) (0.1.2)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.68.2->litellm>=1.53.1->crawl4ai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.68.2->litellm>=1.53.1->crawl4ai)\n",
      "  Downloading jiter-0.9.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from openai>=1.68.2->litellm>=1.53.1->crawl4ai) (1.3.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from tokenizers->litellm>=1.53.1->crawl4ai) (0.30.1)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography<45,>=41.0.5->pyOpenSSL>=24.3.0->crawl4ai)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/bert/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (6.0.2)\n",
      "Downloading crawl4ai-0.5.0.post8-py3-none-any.whl (312 kB)\n",
      "Using cached aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
      "Downloading fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n",
      "Downloading faust_cchardet-2.1.19-cp311-cp311-macosx_11_0_arm64.whl (134 kB)\n",
      "Downloading humanize-4.12.2-py3-none-any.whl (128 kB)\n",
      "Downloading litellm-1.66.1-py3-none-any.whl (7.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached pillow-10.4.0-cp311-cp311-macosx_11_0_arm64.whl (3.4 MB)\n",
      "Downloading pyOpenSSL-25.0.0-py3-none-any.whl (56 kB)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Using cached snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\n",
      "Downloading tf_playwright_stealth-1.1.2-py3-none-any.whl (33 kB)\n",
      "Downloading cryptography-44.0.2-cp39-abi3-macosx_10_9_universal2.whl (6.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fake_http_header-0.3.5-py3-none-any.whl (14 kB)\n",
      "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Downloading openai-1.75.0-py3-none-any.whl (646 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.0/647.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-macosx_11_0_arm64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cffi-1.17.1-cp311-cp311-macosx_11_0_arm64.whl (178 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.9.0-cp311-cp311-macosx_11_0_arm64.whl (320 kB)\n",
      "Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.24.0-cp311-cp311-macosx_11_0_arm64.whl (362 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Building wheels for collected packages: pyperclip\n",
      "  Building wheel for pyperclip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyperclip: filename=pyperclip-1.9.0-py3-none-any.whl size=11102 sha256=ffa9a84c9b10302e170588bef9e503b586fc8950b382495f96fa68e3b7dff156\n",
      "  Stored in directory: /Users/binbasri0/Library/Caches/pip/wheels/e8/e7/56/591cb88ba1783b38c40d584026e766aac9c3a048e34128ce8b\n",
      "Successfully built pyperclip\n",
      "Installing collected packages: snowballstemmer, pyperclip, faust-cchardet, rpds-py, rank-bm25, pycparser, pillow, nltk, jiter, humanize, fake-useragent, fake-http-header, distro, cssselect, colorama, aiosqlite, aiofiles, tiktoken, referencing, cffi, tf-playwright-stealth, openai, jsonschema-specifications, cryptography, pyOpenSSL, jsonschema, litellm, crawl4ai\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 11.1.0\n",
      "    Uninstalling pillow-11.1.0:\n",
      "      Successfully uninstalled pillow-11.1.0\n",
      "Successfully installed aiofiles-24.1.0 aiosqlite-0.21.0 cffi-1.17.1 colorama-0.4.6 crawl4ai-0.5.0.post8 cryptography-44.0.2 cssselect-1.3.0 distro-1.9.0 fake-http-header-0.3.5 fake-useragent-2.2.0 faust-cchardet-2.1.19 humanize-4.12.2 jiter-0.9.0 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 litellm-1.66.1 nltk-3.9.1 openai-1.75.0 pillow-10.4.0 pyOpenSSL-25.0.0 pycparser-2.22 pyperclip-1.9.0 rank-bm25-0.2.2 referencing-0.36.2 rpds-py-0.24.0 snowballstemmer-2.2.0 tf-playwright-stealth-1.1.2 tiktoken-0.9.0\n",
      "\u001b[36m[INIT].... → Running post-installation setup...\u001b[0m\n",
      "\u001b[36m[INIT].... → Installing Playwright browsers...\u001b[0m\n",
      "\u001b[32m[COMPLETE] ● Playwright installation completed successfully.\u001b[0m\n",
      "\u001b[36m[INIT].... → Starting database initialization...\u001b[0m\n",
      "\u001b[36m[COMPLETE] ● Database backup created at: /Users/binbasri0/.crawl4ai/crawl4ai.db.backup_20250417_102038\u001b[0m\n",
      "\u001b[36m[INIT].... → Starting database migration...\u001b[0m\n",
      "\u001b[32m[COMPLETE] ● Migration completed. 0 records processed.\u001b[0m\n",
      "\u001b[32m[COMPLETE] ● Database initialization completed successfully.\u001b[0m\n",
      "\u001b[32m[COMPLETE] ● Post-installation setup completed!\u001b[0m\n",
      "\u001b[0m\u001b[36m[INIT].... → Running Crawl4AI health check...\u001b[0m\n",
      "\u001b[36m[INIT].... → Crawl4AI 0.5.0.post8\u001b[0m\n",
      "\u001b[36m[TEST].... ℹ Testing crawling capabilities...\u001b[0m\n",
      "\u001b[36m[EXPORT].. ℹ Exporting PDF and taking screenshot took 1.20s\u001b[0m\n",
      "\u001b[32m[FETCH]... ↓ https://crawl4ai.com... | Status: \u001b[32mTrue\u001b[0m | Time: 5.40s\u001b[0m\n",
      "\u001b[36m[SCRAPE].. ◆ https://crawl4ai.com... | Time: 0.023s\u001b[0m\n",
      "\u001b[32m[COMPLETE] ● https://crawl4ai.com... | Status: \u001b[32mTrue\u001b[0m | Total: \u001b[33m5.42s\u001b[0m\u001b[0m\n",
      "\u001b[32m[COMPLETE] ● ✅ Crawling test passed!\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U crawl4ai\n",
    "!crawl4ai-setup\n",
    "!crawl4ai-doctor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
